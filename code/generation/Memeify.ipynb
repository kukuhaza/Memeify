{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Memeify_3",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqWp4rxv-_Xl",
        "colab_type": "text"
      },
      "source": [
        "# **Utilities**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from nltk.tag import pos_tag\n",
        "import re\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUmTooTW3osf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab_type": "code",
        "outputId": "6c4a6438-c4b9-4811-98ac-a39374d68838",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"117M\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 261Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 14.2Mit/s]                                                   \n",
            "Fetching hparams.json: 1.05Mit [00:00, 904Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:02, 178Mit/s]                                   \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 284Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 135Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 137Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puq4iC6vUAHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyZR6FVeNEiL",
        "colab_type": "code",
        "outputId": "77982160-187b-4714-be49-51a74794a729",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_name = \"final_captions_with_classes.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6okFD8VKtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_file_from_gdrive(file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuxvkFxI-ta-",
        "colab_type": "text"
      },
      "source": [
        "# **Finetuning GPT-2 model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab_type": "code",
        "outputId": "6e3c7238-5b6b-44b3-e399-ba1dac5b3b4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='117M',\n",
        "              steps=2000,\n",
        "              restore_from='latest',\n",
        "              run_name='run3',\n",
        "              print_every=5,\n",
        "              sample_every=200,\n",
        "              save_every=300\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint checkpoint/run3/model-1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 1/1 [01:25<00:00, 85.79s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 17750868 tokens\n",
            "Training...\n",
            "======== SAMPLE 1 ========\n",
            " Dubries\n",
            "insurgent_bookseller_star.txt I can buy your books I've already seen them in the bookstore \n",
            "insurgent_bookseller_star.txt Oh! You want the latest in the field of classics?  Oh, you have a copy or two? \n",
            "insurgent_bookseller_star.txt You like reading in bed? I'll give you a massage.\n",
            "insurgent_bookseller_star.txt My favorite book? That's from the novel by David Mitchell\n",
            "insurgent_bookseller_star.txt Oh no you can't order the book I've only got one copy of that book in the back  you can read without the book\n",
            "insurgent_bookseller_star.txt Ohh, you have these \"best-seller-closet\" posters for sale?  I love my art on it\n",
            "insurgent_bookseller_star.txt Oh, you're buying a paperback book every year? Well, I'll give you that book I found in the book fair  every year on the seventh floor of the New Library\n",
            "insurgent_bookseller_star.txt Oh, you want the science fiction title to be just like that in the book?  Here, I'll tell you the plot of the book, so you can appreciate what really happened\n",
            "insurgent_bookseller_star.txt oh, you can pay a little more for the Harry Potter books?   ...\n",
            "insurgent_bookseller_star.txt oh, you want all the free stuff! You must see this shit\n",
            "insurgent_bookseller_star.txt Oh, you want a 20\" \"l\" copy of the new books every week? I'll give you an 8\"\n",
            "insurgent_bookseller_star.txt Oh so she's going to hate me for asking? She's a fucking cat\n",
            "insurgent_bookseller_star.txt Oh, you need to change your Facebook status to read in the comments? Don't worry I'll give you a quick face\n",
            "insurgent_bookseller_star.txt Hint, hint: This book is not even in the title yet\n",
            "insurgent_bookseller_star.txt How was your birthday?  Oh, you're looking for it all.\n",
            "insurgent_bookseller_star.txt i have a good grasp of how to sell a book on its own, but i can't really explain all the details\n",
            "insurgent_bookseller_star.txt Oh, you have a book of poetry under your bed? You better put it in the back and wait until it's about to be pooped in\n",
            "insurgent_bookseller_star.txt Oh, you have a book of poetry under your bed? Then just give me a blow job\n",
            "insurgent_bookseller_star.txt Oh, You had the copy for the last book of the year? I'll give you a break from reading\n",
            "insurgent_bookseller_star.txt Oh, You want to go to a book dealer for the upcoming books?  So can i look through the entire line in the back\n",
            "insurgent_bookseller_star.txt Oh, you want books with an explanation of why science has failed to explain evolution?  Here, I'll explain evolution in reverse order\n",
            "insurgent_bookseller_star.txt HMMM mr. harry can do better than me! \n",
            "insurgent_bookseller_star.txt hmmm, there you go, you will go.            you are the good old days. \n",
            "insurgently_oblivious_bookseller_star.txt You can't find a copy of this book in the bookstore? Well, here are five of my favorite stories\n",
            "insurgent_bookseller_star.txt Oh, you need money to buy the books at the big box store? Well, here are my favorite episodes\n",
            "insurgent_bookseller_star.txt you want to check out all of these great new books\n",
            "insurgent_bookseller_star.txt Oh, You want to buy a new book from me? I am so excited that I am the one who has your back\n",
            "insurgent_bookseller_star.txt *screams to the outside of your room* *cries, \"Hey, hear me out\"* How was your shower last time you were out? Good.  \n",
            "insurgent_bookseller_star.txt Oh, you like to watch movies? I've got a list of movies I have watched.  Just put the movies in the Movies and then click back out\n",
            "insurgent_bookseller_star.txt I have to tell you about a great discovery that I found in The Twilight zone\n",
            "insurgent_bookseller_star.txt Oh you need to see\n",
            "\n",
            "[1005 | 29.41] loss=2.54 avg=2.54\n",
            "[1010 | 41.04] loss=2.67 avg=2.61\n",
            "[1015 | 52.87] loss=2.18 avg=2.46\n",
            "[1020 | 64.71] loss=3.03 avg=2.61\n",
            "[1025 | 76.37] loss=2.40 avg=2.57\n",
            "[1030 | 87.91] loss=2.73 avg=2.59\n",
            "[1035 | 99.40] loss=2.40 avg=2.57\n",
            "[1040 | 110.86] loss=2.60 avg=2.57\n",
            "[1045 | 122.35] loss=2.19 avg=2.53\n",
            "[1050 | 133.93] loss=2.36 avg=2.51\n",
            "[1055 | 145.57] loss=2.53 avg=2.51\n",
            "[1060 | 157.22] loss=2.99 avg=2.55\n",
            "[1065 | 168.84] loss=2.83 avg=2.58\n",
            "[1070 | 180.46] loss=2.25 avg=2.55\n",
            "[1075 | 192.07] loss=2.60 avg=2.56\n",
            "[1080 | 203.64] loss=2.66 avg=2.56\n",
            "[1085 | 215.21] loss=2.94 avg=2.59\n",
            "[1090 | 226.78] loss=2.36 avg=2.57\n",
            "[1095 | 238.35] loss=2.64 avg=2.58\n",
            "[1100 | 249.94] loss=2.75 avg=2.59\n",
            "[1105 | 261.51] loss=2.44 avg=2.58\n",
            "[1110 | 273.09] loss=2.25 avg=2.56\n",
            "[1115 | 284.67] loss=2.01 avg=2.54\n",
            "[1120 | 296.24] loss=2.52 avg=2.53\n",
            "[1125 | 307.82] loss=2.72 avg=2.54\n",
            "[1130 | 319.40] loss=2.56 avg=2.54\n",
            "[1135 | 331.01] loss=3.04 avg=2.56\n",
            "[1140 | 342.63] loss=2.86 avg=2.58\n",
            "[1145 | 354.25] loss=2.47 avg=2.57\n",
            "[1150 | 365.86] loss=2.76 avg=2.58\n",
            "[1155 | 377.47] loss=2.48 avg=2.58\n",
            "[1160 | 389.11] loss=1.97 avg=2.55\n",
            "[1165 | 400.74] loss=2.58 avg=2.55\n",
            "[1170 | 412.37] loss=2.44 avg=2.55\n",
            "[1175 | 424.01] loss=2.19 avg=2.54\n",
            "[1180 | 435.63] loss=2.59 avg=2.54\n",
            "[1185 | 447.25] loss=2.34 avg=2.53\n",
            "[1190 | 458.87] loss=2.85 avg=2.54\n",
            "[1195 | 470.49] loss=2.43 avg=2.54\n",
            "[1200 | 482.09] loss=2.55 avg=2.54\n",
            "Saving checkpoint/run3/model-1200\n",
            "======== SAMPLE 1 ========\n",
            " bands have their own opinions and opinions. They're just different people; you have your opinions, you will disagree with them. You know, you can take a picture of yourself, and it'll all be taken down, you don't have any right to post it. \n",
            "jj_jameson.txt Hi i'm kyle chase  i'm so high!\n",
            "jj_jameson.txt so you say  your girlfriend is a lesbian? you mean like just give her some lesbians? and give her an orgasm instead of just getting fucked by the dick! \n",
            "jj_jameson.txt Do you do yoga?  I don't do yoga. \n",
            "jj_jameson.txt Where should I buy my tickets for next season of york? I have a ticket and a ticket but this is gonna be tough! \n",
            "jj_jameson.txt So you have a brother, right? I'm not really that good at basketball so I don't know if it suits me to your taste, But you know it suits me to you! \n",
            "jj_jameson.txt so how ya you play basketball? uhh...i just like your best games, cause uhh...I'm pretty good at basketball  \n",
            "jj_jameson.txt So your dad was a teacher in '90s and '00s So you saw his high school teacher work at school? \n",
            "jj_jameson.txt Have you ever heard of Rontez? Me neither... Yes! \n",
            "jj_jameson.txt  So what's your relationship like with the girl who just got rid of you?  Yeah, but it isn't as nice as your divorce. i mean look at just how big it is! \n",
            "jj_jameson.txt Whats it like to look like a douchebag  like you did in highschool?  \n",
            "jj_jameson.txt So today you found out that your father had prostate cancer?      It's my dad. I'm not going to let him down \n",
            "jj_jameson.txt so we were talking about the new Batman movie at this week's reunion... i cant wait to see the face of jacob bailiff i hope he got his new goth \n",
            "jj_jameson.txt your parents didn't give you a seat but you can stand by and listen to your mother  because she has a private jet  \n",
            "jj_jameson.txt So what is a better driver than a horse? The horse who got the fastest lap in the 400!!! I'VE READ that before you ask yourself what's worse,                                                                                                   \n",
            "jj_jameson.txt Did you just play hide-and-seek? no i didnt \n",
            "jj_jameson.txt you know your gonna have to ask your mum about what did you eat last night ? no i ate my sister\n",
            "jj_jameson.txt So, i heard your mom is taking her son to see a theatre in central park the other night.  Wait until you hear her saying \"you go to the mall, you go to the theatre\". \n",
            "jj_jameson.txt you want to see a movie of that glee show today.. what movie.. \n",
            "jj_jameson.txt so what's the best way to make friends with guys like you? you had a friend in high school called jackie. \n",
            "jj_jameson.txt So you're from kabali.  How did you get you mama's top? I got my mama from an Asian guy \n",
            "jj_jameson.txt You saw some of tha old british movies in british now? they're so good. \n",
            "jj_jameson.txt so you work at your dad's restaurant... and it tastes like shit  \n",
            "jj_jameson.txt So do we know why they're sending you to a college in Europe this year? because they want you to get a degree in Europe. i've met a guy from America. \n",
            "jj_jameson.txt hey there peter its a bitch you know that's right, you're right, its not rape because  i don't rape you \n",
            "jj_jameson.txt your girlfriend is pregnant and she's like a little bitch and i didn't even know she was pregnant you know, that's right i DIDT\n",
            "\n",
            "[1205 | 507.84] loss=2.91 avg=2.55\n",
            "[1210 | 519.45] loss=2.39 avg=2.55\n",
            "[1215 | 531.07] loss=2.38 avg=2.54\n",
            "[1220 | 542.67] loss=2.38 avg=2.54\n",
            "[1225 | 554.27] loss=2.45 avg=2.54\n",
            "[1230 | 565.85] loss=2.72 avg=2.54\n",
            "[1235 | 577.44] loss=2.50 avg=2.54\n",
            "[1240 | 589.02] loss=2.31 avg=2.53\n",
            "[1245 | 600.59] loss=2.75 avg=2.54\n",
            "[1250 | 612.18] loss=2.74 avg=2.54\n",
            "[1255 | 623.76] loss=2.80 avg=2.55\n",
            "[1260 | 635.34] loss=2.73 avg=2.55\n",
            "[1265 | 646.91] loss=2.45 avg=2.55\n",
            "[1270 | 658.49] loss=2.68 avg=2.56\n",
            "[1275 | 670.06] loss=2.66 avg=2.56\n",
            "[1280 | 681.62] loss=2.43 avg=2.55\n",
            "[1285 | 693.18] loss=2.70 avg=2.56\n",
            "[1290 | 704.74] loss=2.33 avg=2.55\n",
            "[1295 | 716.29] loss=2.14 avg=2.54\n",
            "[1300 | 727.84] loss=2.59 avg=2.54\n",
            "[1305 | 739.39] loss=2.17 avg=2.54\n",
            "[1310 | 750.94] loss=2.54 avg=2.54\n",
            "[1315 | 762.50] loss=2.28 avg=2.53\n",
            "[1320 | 774.07] loss=2.22 avg=2.52\n",
            "[1325 | 785.64] loss=2.17 avg=2.52\n",
            "[1330 | 797.23] loss=2.59 avg=2.52\n",
            "[1335 | 808.82] loss=2.30 avg=2.51\n",
            "[1340 | 820.43] loss=2.22 avg=2.51\n",
            "[1345 | 832.04] loss=1.96 avg=2.50\n",
            "[1350 | 843.65] loss=2.28 avg=2.49\n",
            "[1355 | 855.25] loss=2.47 avg=2.49\n",
            "[1360 | 866.84] loss=2.71 avg=2.50\n",
            "[1365 | 878.44] loss=1.94 avg=2.49\n",
            "[1370 | 890.06] loss=2.28 avg=2.48\n",
            "[1375 | 901.63] loss=2.50 avg=2.48\n",
            "[1380 | 913.21] loss=2.53 avg=2.48\n",
            "[1385 | 924.79] loss=2.14 avg=2.48\n",
            "[1390 | 936.38] loss=2.69 avg=2.48\n",
            "[1395 | 948.00] loss=2.51 avg=2.48\n",
            "[1400 | 959.59] loss=2.42 avg=2.48\n",
            "======== SAMPLE 1 ========\n",
            "oul_greg your life is like my mom I'm the fuck who you are not my mom you're fucking fucked yeah, like a chicken fuck yeah\n",
            "unhelpful_high_school_teacher.txt  That's the thing you learned.\n",
            "unhelpful_high_school_teacher.txt tells you to look out your window to see the sunset.  Makes this as much of a date as possible\n",
            "unhelpful_high_school_teacher.txt The amount of time I've been teaching you how to use your hand. is too damn high. I don't have.\n",
            "unhelpful_high_school_teacher.txt  If someone was to get mad at us we would have the last laugh; that being an idiot would give them a second chance\n",
            "unhelpful_high_school_teacher.txt I have an assignment on and you're due on 6/15\n",
            "unhelpful_high_school_teacher.txt  I'll give you 10% off first class homework\n",
            "unhelpful_high_school_teacher.txt I don't get it! The teacher is out today so don't think about her tomorrow! \n",
            "unhelpful_high_school_teacher.txt \"I don't know, how did you come up with all that data?\"   You know, because you know. \n",
            "unhelpful_high_school_teacher.txt Can i go to class? I thought you got an all-you-can-eat plate\n",
            "unhelpful_high_school_teacher.txt Ask questions on the front page of r/trees you've only ever thought of\n",
            "unhelpful_high_school_teacher.txt You don't read the paper? I can see that your lack of understanding is a learning handicap.\n",
            "unhelpful_high_school_teacher.txt \"You don't know the answer to this question, no matter how much I ask\" Is that what you wanted to write me down?\n",
            "unhelpful_high_school_teacher.txt \"Can I go to the bathroom?\" I don't know... can you?\n",
            "unhelpful_high_school_teacher.txt You don't know the answer to this question. Can you explain that as well as I can?\n",
            "unhelpful_high_school_teacher.txt \"I'll have my homework done by the end of the week so no homework left.\"  I told you when you finished your homework!\n",
            "unhelpful_high_school_teacher.txt \"Does she teach self-discipline?\" No, because that's not what she teaches.\n",
            "unhelpful_high_school_teacher.txt Ask questions \"I only talk to students who know what I'm talking about.\"\n",
            "unhelpful_high_school_teacher.txt Student asks a question for a class, she can't respond \"I can do it myself\" Is she really retarded\n",
            "unhelpful_high_school_teacher.txt \"I don't know where this is going, I think we should be using computers instead.\"  I can't program!\n",
            "unhelpful_high_school_teacher.txt Do your homework. We need to figure out how we should grade it. \n",
            "unhelpful_high_school_teacher.txt \"That's a question!\"  No, that's a question.\n",
            "unhelpful_high_school_teacher.txt \"Don't forget to turn off the television.\"  I used to to though\n",
            "unhelpful_high_school_teacher.txt Teaches class about to be born deaf to the teachers. Can't use it any louder than that? I don't know. Can you?\n",
            "unhelpful_high_school_teacher.txt \"Teacher, check your assignment on time and see if nobody has left.\" No, I'm out of Time.\n",
            "unhelpful_high_school_teacher.txt \"It is my final exam so please get the last one out before 5:30.\" \"5:30pm\"? Get the last one out before 5:30pm! This is not my final exam!\"\n",
            "unhelpful_high_school_teacher.txt Gives you a lecture about what no one asked. Doesn't answer questions based on the answers.\n",
            "unhelpful_high_school_teacher.txt I'm going to need you to sign it in this class\n",
            "unhelpful_high_school_teacher.txt asks student to \"pick up some tissues\" doesn't buy tissues because he isn't comfortable with the pain\n",
            "unhelpful_high_school_teacher.txt You don't know what \"C\" means? So... you don't know what \"A\" means? I don't know, I don't know, you don't know?\n",
            "unhelp\n",
            "\n",
            "[1405 | 982.28] loss=2.35 avg=2.48\n",
            "[1410 | 993.82] loss=2.15 avg=2.47\n",
            "[1415 | 1005.34] loss=2.32 avg=2.47\n",
            "[1420 | 1016.89] loss=2.56 avg=2.47\n",
            "[1425 | 1028.45] loss=2.36 avg=2.47\n",
            "[1430 | 1040.02] loss=2.32 avg=2.47\n",
            "[1435 | 1051.61] loss=2.54 avg=2.47\n",
            "[1440 | 1063.20] loss=2.14 avg=2.46\n",
            "[1445 | 1074.83] loss=2.11 avg=2.46\n",
            "[1450 | 1086.45] loss=2.24 avg=2.45\n",
            "[1455 | 1098.09] loss=2.76 avg=2.46\n",
            "[1460 | 1109.72] loss=2.12 avg=2.45\n",
            "[1465 | 1121.32] loss=2.24 avg=2.45\n",
            "[1470 | 1132.93] loss=2.29 avg=2.45\n",
            "[1475 | 1144.53] loss=2.36 avg=2.44\n",
            "[1480 | 1156.14] loss=2.73 avg=2.45\n",
            "[1485 | 1167.74] loss=2.57 avg=2.45\n",
            "[1490 | 1179.33] loss=2.23 avg=2.45\n",
            "[1495 | 1190.93] loss=2.37 avg=2.45\n",
            "[1500 | 1202.53] loss=2.45 avg=2.45\n",
            "Saving checkpoint/run3/model-1500\n",
            "[1505 | 1216.77] loss=2.13 avg=2.44\n",
            "[1510 | 1228.39] loss=2.44 avg=2.44\n",
            "[1515 | 1240.05] loss=2.47 avg=2.44\n",
            "[1520 | 1251.67] loss=2.78 avg=2.45\n",
            "[1525 | 1263.26] loss=2.18 avg=2.44\n",
            "[1530 | 1274.85] loss=2.25 avg=2.44\n",
            "[1535 | 1286.46] loss=2.33 avg=2.44\n",
            "[1540 | 1298.05] loss=2.47 avg=2.44\n",
            "[1545 | 1309.62] loss=2.44 avg=2.44\n",
            "[1550 | 1321.21] loss=2.31 avg=2.44\n",
            "[1555 | 1332.81] loss=2.40 avg=2.44\n",
            "[1560 | 1344.39] loss=2.23 avg=2.43\n",
            "[1565 | 1355.97] loss=2.38 avg=2.43\n",
            "[1570 | 1367.56] loss=2.36 avg=2.43\n",
            "[1575 | 1379.12] loss=2.36 avg=2.43\n",
            "[1580 | 1390.69] loss=2.64 avg=2.43\n",
            "[1585 | 1402.25] loss=1.99 avg=2.43\n",
            "[1590 | 1413.82] loss=1.97 avg=2.42\n",
            "[1595 | 1425.41] loss=2.45 avg=2.42\n",
            "[1600 | 1436.99] loss=2.40 avg=2.42\n",
            "======== SAMPLE 1 ========\n",
            " sleep and have to put on clothes for the walk has no shoes\n",
            "scumbag_brain.txt Knows a song is too good to play constantly forgets after sleep when you're already awake\n",
            "scumbag_brain.txt Knows you've got no future ahead.              You're going to get yourself an M.O. in college.\n",
            "scumbag_brain.txt \"Oh, you're back to being a normal human being? Let's wake up.\" Wake up \"forever\"\n",
            "scumbag_brain.txt Wants to wake up so you know which direction you're going.                                                                              \"Sole night,  wake up as usual.\"\n",
            "scumbag_brain.txt Have to go to sleep Pretend to not need to go for that long distance\n",
            "scumbag_brain.txt Loves to masturbates to  \"That's all I need.\"\n",
            "scumbag_brain.txt Oh, you finally got up? Let's just pray that you don't get too tired. Then do your best work\n",
            "scumbag_brain.txt Oh that's a sweet, sweet bj you really just had this good dream all night that you were too beautiful to fuck\n",
            "scumbag_brain.txt Doesn't want to learn anything new Or study new\n",
            "scumbag_brain.txt Remember what you're doing? Let me just imagine all your problems.  \n",
            "scumbag_brain.txt you're at a great college party? Let's think of your social life.  \n",
            "scumbag_brain.txt Just a dream? Let's dream about all of us.\n",
            "scumbag_brain.txt Makes you appreciate something completely forgets about you and your friend, thus avoiding future potential friends\n",
            "scumbag_brain.txt Your girlfriend's boyfriend is in a relationship?  What a dream: you're alone  You're out of boyfriends: remember everything and get back to school now  \n",
            "scumbag_brain.txt Wakes you up at 3 AM every morning to do homework, spend the day with friends Doesn't remember anyone you meet in the middle of the night\n",
            "scumbag_brain.txt Ohh, you have to get up and go to sleep? I heard you're awake at 3 am. \n",
            "scumbag_brain.txt Oh, you've been working out and you feel like you're not that great? Let me just say you would probably get really sick if you didn't work out\n",
            "scumbag_brain.txt Knows you're going through the wrong phase of a memory remembers at least two of the other four\n",
            "scumbag_brain.txt Hey, remember our last song? Remember it, as often as you want.\n",
            "scumbag_brain.txt Oh, you're totally excited about a new song? Go ahead, sing it a minute later. Just don't. \n",
            "scumbag_brain.txt You're going to hell? Let's fantasize about that.\n",
            "scumbag_brain.txt I don't go to sleep until i'm tired, but tonight you will never get any sleep \n",
            "scumbag_brain.txt you've never looked anything a little cute on facebook in your life? let's imagine how you would wish to feel to have such a unique experience\n",
            "scumbag_brain.txt Oh, you're really hungry?  I can totally see that all night you're asleep\n",
            "scumbag_brain.txt I have a dream about a girl that you love Let's dream about her\n",
            "scumbag_brain.txt Hey baby, guess what is wrong with your face?  I bet you have a sore throat. \n",
            "scumbag_brain.txt Hey, remember the last time you took a nap?  Don't. Forget.  \n",
            "scumbag_brain.txt You're home alone? Let's masturbate to something you really hate and hope it makes you climax the right way\n",
            "scumbag_brain.txt You have a horrible memory? Now imagine what it's like to have a vivid vivid dream that you were once a good boy\n",
            "scumbag_brain.txt  Hey, that was your best movie of the year? Remember that, too?\n",
            "scumbag_brain.txt Yeah, you have to go to bed.  But I'll wake up early \n",
            "scumbag_brain.txt you think you know me. what do you look like?\n",
            "scumbag_brain.txt Oh,\n",
            "\n",
            "[1605 | 1459.39] loss=2.24 avg=2.42\n",
            "[1610 | 1470.93] loss=2.76 avg=2.42\n",
            "[1615 | 1482.46] loss=2.55 avg=2.42\n",
            "[1620 | 1494.02] loss=2.41 avg=2.42\n",
            "[1625 | 1505.59] loss=2.20 avg=2.42\n",
            "[1630 | 1517.17] loss=2.37 avg=2.42\n",
            "[1635 | 1528.75] loss=2.30 avg=2.42\n",
            "[1640 | 1540.36] loss=2.61 avg=2.42\n",
            "[1645 | 1551.96] loss=2.37 avg=2.42\n",
            "[1650 | 1563.57] loss=2.10 avg=2.42\n",
            "[1655 | 1575.19] loss=2.28 avg=2.41\n",
            "[1660 | 1586.79] loss=2.24 avg=2.41\n",
            "[1665 | 1598.40] loss=2.30 avg=2.41\n",
            "[1670 | 1610.02] loss=2.63 avg=2.41\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHdTL8NDbAh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name='run3')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fxL77nvAMAX",
        "colab_type": "code",
        "outputId": "c8b1605c-98ef-4812-e9b8-665f7fada00e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='run3')\n",
        "# gpt2.load_gpt2(sess, run_name='run1')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint checkpoint/run3/model-2000\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/run3/model-2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RuFiM5B-0f-",
        "colab_type": "text"
      },
      "source": [
        "# **Sampling new memes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "colab_type": "code",
        "outputId": "b276c1cd-95ba-423e-ffd9-ead0e6b614c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "gpt2.generate(sess, run_name='run3')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> i dont always make a meme but when i do i immediately press th<end>\n",
            "<start> i dont always generate a meme but when i do i generate them tw<end>\n",
            "<start> i dont always generate a meme but when i do i generate them tw<end>\n",
            "<start> i dont always talk to arts students but when i do i ask for la<end>\n",
            "<start> i dont always launch internet explorer but when i do its by ac<end>\n",
            "<start> i dont always use internet explorer but when i do its to downl<end>\n",
            "<start> i dont always watch twilight but when i do i dont<end>\n",
            "<start> i dont always lose my phone but when i do its on silent mode<end>\n",
            "<start> i dont always contradict myself but when i do i dont<end>\n",
            "<start> i dont always fart but when i do i lean like this<end>\n",
            "<start> i dont always do homework but when i do i wait until the last <end>\n",
            "<start> i dont always correct peoples grammar but when i do i google t<end>\n",
            "<start> i dont always drink milk but when i do i prefer dos titties<end>\n",
            "<start> i dont always use fury swipes but when i do it hits five times<end>\n",
            "<start> i dont always finish my sentences but when i do<end>\n",
            "<start> i dont always make a good meme this is a perfect example<end>\n",
            "<start> i dont always pay attention to typos that other people make in<end>\n",
            "<start> i dont always have alzheimers i dont always have alzheimers<end>\n",
            "<start> i dont always open internet explorer but when i do its to let <end>\n",
            "<start> i dont always text you but when i do i make sure its 2 am and <end>\n",
            "<start> i dont always see my crush but when i do i look like shit<end>\n",
            "<start> i dont always rewind a youtube video but when i do it has to r<end>\n",
            "<start> but when i do you can see my perfect hair<end>\n",
            "<start> i dont always go to mordor but when i do i simply walk in<end>\n",
            "<start> i dont always run out of toilet paper but when i do i have dia<end>\n",
            "<start> i dont always use internet explorer but when i do im downloadi<end>\n",
            "<start> i dont always use bing but when i do i use it to search for go<end>\n",
            "<start> i dont always think about you but when i do i touch myself<end>\n",
            "<start> i dont alwals finish sentences <end>\n",
            "<start> i dont always play video games that destroy my social life but<end>\n",
            "<start> i dont always die in skyrim but when i do i havent saved in 3 <end>\n",
            "<start> i dont always use the wrong meme challenge accepted<end>\n",
            "<start> i dont always call 911 but when i do i ask the operator what s<end>\n",
            "<start> i dont always see but when i do its what you did there<end>\n",
            "<start> i dont always post on memegenerator but when i do its because <end>\n",
            "<start> i dont always talk to ohio state graduates but when i do i ask<end>\n",
            "<start> i dont always laugh at memes but when i do they are racist<end>\n",
            "<start> i dont usually fart but when i do i lean like this<end>\n",
            "<start> i dont always get up with time to spare but when i do i waste <end>\n",
            "<start> i dont always finish my sentences but when i do<end>\n",
            "<start> i dont always tell my girlfriend i love her but when i do its <end>\n",
            "<start> i dont always fap but when i do you cant see my left hand<end>\n",
            "<start> i dont always scratch my balls but when i do i smell my hands<end>\n",
            "<start> i dont always cry but when i do its because angels deserve to <end>\n",
            "<start> i dont always put my laptop on my lap but when i do i think im<end>\n",
            "<start> i do it right i dont always time travel but when i do<end>\n",
            "<start> i dont always make bad memes but when i do i cant delete them<end>\n",
            "<start> i dont always get drunk but when i do i wake up in a strange p<end>\n",
            "<start> i dont always make millions off of scams but when i do i mastr<end>\n",
            "<start> i dont always use bing but when i do\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_captions_string=list(gpt2.generate(sess,\n",
        "              length=100,\n",
        "              temperature=1,\n",
        "              nsamples=500,\n",
        "              batch_size=500,\n",
        "              run_name='run3',\n",
        "              return_as_list=True\n",
        "              ))\n",
        "\n",
        "# prefix='high_expectations_asian_father'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alh9wsQ4U9Dj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_captions_string='\\n'.join(all_captions_string[i] for i in range(len(all_captions_string)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjrW0pqkWwVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code to generate and split caption for meme image\n",
        "def get_captions_from_class(all_captions_string, class_name):\n",
        "  if('.' in class_name):\n",
        "    first_occ=all_captions_string.find(class_name[:classname.find('.')])\n",
        "    last_occ=all_captions_string.rfind(class_name[:classname.find('.')])\n",
        "  else:\n",
        "    first_occ=all_captions_string.find(class_name)\n",
        "    last_occ=all_captions_string.rfind(class_name)  \n",
        "  print(first_occ, last_occ)\n",
        "  if(first_occ==-1):\n",
        "    return []\n",
        "#   print(all_captions_string[first_occ:last_occ+len(class_name)], len(all_captions_string[first_occ:last_occ+len(class_name)]))\n",
        "  splitted_list=all_captions_string[first_occ:last_occ+len(class_name)].split('\\n')\n",
        "  return splitted_list\n",
        "    \n",
        "  \n",
        "def split_sentence(sentence, class_name):\n",
        "  \n",
        "  if('.txt' in sentence):\n",
        "    sentence=sentence[len(class_name)+4:]\n",
        "  elif('.md' in sentence):\n",
        "    sentence=sentence[len(class_name)+3:]\n",
        "  else:\n",
        "    sentence=sentence[len(class_name):]\n",
        "  \n",
        "  tagged_sent=pos_tag(sentence.lower().split())\n",
        "#   print(tagged_sent)\n",
        "#   print(sentence+' cropped')\n",
        "  if('imminent_ned' in class_name):\n",
        "    if('brace' in sentence.lower()):\n",
        "      sentence_low=sentence.lower()\n",
        "      break_point=sentence_low.find('brace yourselves')+len('brace yourselves')\n",
        "    \n",
        "      if(break_point!=-1):\n",
        "        first_part=sentence[:break_point]\n",
        "        second_part=sentence[break_point:]\n",
        "        return first_part, second_part\n",
        "      else:\n",
        "        break_point=sentence.find('brace yourself')+len('brace yourself') \n",
        "        if(break_point!=-1):\n",
        "          first_part=sentence[:break_point]\n",
        "          second_part=sentence[break_point:]\n",
        "          return first_part, second_part\n",
        "        else:\n",
        "          first_part='brace yourselves'\n",
        "          second_part=sentence\n",
        "          return first_part, second_part\n",
        "    else:\n",
        "      first_part='brace yourselves'\n",
        "      second_part=sentence\n",
        "      return first_part, second_part\n",
        "    \n",
        "  else:\n",
        "    break_point=sentence.find('?')\n",
        "    if(break_point!=-1):\n",
        "      first_part=sentence[:break_point+1]\n",
        "      second_part=sentence[break_point+1:]\n",
        "      return first_part, second_part\n",
        "    else:\n",
        "      sentence_list=sentence.split(' ')\n",
        "#       print(tagged_sent)\n",
        "      sentence_list=list(filter(lambda a: a!='', sentence_list))\n",
        "      \n",
        "#       print(sentence_list)\n",
        "\n",
        "      break_point=0\n",
        "      for i in range(1, len(tagged_sent)):\n",
        "        word=sentence_list[i]\n",
        "        tag=tagged_sent[i][1]\n",
        "        \n",
        "        if(word[0].isupper()):\n",
        "          if(tag!='NNP' and tag!='NN' and tag!='NNS' and tag!='NNPS'):\n",
        "            print(word)\n",
        "            break_point=i\n",
        "            break\n",
        "#       print(break_point)\n",
        "      if(break_point!=0):\n",
        "        first_part=' '.join(sentence_list[ite] for ite in range(break_point))\n",
        "        second_part=' '.join(sentence_list[ite] for ite in range(break_point, len(sentence_list)))\n",
        "        return first_part, second_part\n",
        "      else:\n",
        "        break_point=len(sentence_list)//2\n",
        "        first_part=' '.join(sentence_list[ite] for ite in range(break_point))\n",
        "        second_part=' '.join(sentence_list[ite] for ite in range(break_point, len(sentence_list)))\n",
        "        return first_part, second_part\n",
        "      \n",
        "def get_caption(class_name):\n",
        "  mapping={}\n",
        "  splitted_list=get_captions_from_class(all_captions_string, class_name)\n",
        "  if(len(splitted_list)==0):\n",
        "    print('No memes for: '+class_name)\n",
        "    return []\n",
        "  potentials=[]\n",
        "  k=-1\n",
        "  for i in range(len(splitted_list)):\n",
        "    if(class_name in splitted_list[i] and len(splitted_list[i])>len(class_name)+2):\n",
        "      k+=1\n",
        "      potentials.append(re.sub(' +', ' ', splitted_list[i]))\n",
        "      mapping[k]=k\n",
        "  \n",
        "  potential_selection=[len(p) for p in potentials]\n",
        "  print(potentials)\n",
        "#   best_caption=np.argmax(potential_selection)\n",
        "#   best_caption=mapping[best_caption]\n",
        "  parts=[]\n",
        "  for best_caption in potentials:\n",
        "    sentence=best_caption\n",
        "    #     sentence=potentials[best_caption]\n",
        "    sentence_tokens=sentence.split()\n",
        "    first_part, second_part=split_sentence(sentence, class_name)\n",
        "    print('First and second parts:')\n",
        "    print(first_part)\n",
        "    print(second_part)\n",
        "    parts.append([first_part, second_part])\n",
        "  return parts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz8qTkGiZ9dS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "f=open('/content/drive/My Drive/Memeify dataset/categories.csv', 'r')\n",
        "\n",
        "cats=[x.split(',')[0].lower() for x in list(f.readlines())]\n",
        "\n",
        "for cat in cats:\n",
        "  parts=get_caption(cat)\n",
        "  if(len(parts)>0):\n",
        "    all_memes_pretrained[cat]+=parts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53Z1SyxLYxBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for key in all_memes_pretrained.keys():\n",
        "  print(len(all_memes_pretrained[key]))\n",
        "  \n",
        "import pickle\n",
        "pickle.dump(all_memes_pretrained, open('pretrained_memes.pickle', 'wb'))\n",
        "\n",
        "import shutil\n",
        "shutil.copyfile('pretrained_memes.pickle', \"/content/drive/My Drive/Memeify dataset/pretrained_memes.pickle\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzytnyDbnvwV",
        "colab_type": "code",
        "outputId": "09c0286c-eba1-4ecc-e50a-272cddc4a07e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pretrained_memes=pickle.load(open('/content/drive/My Drive/Memeify dataset/pretrained_memes.pickle', 'rb'))\n",
        "len(pretrained_memes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 321
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDqNZnLVNy0p",
        "colab_type": "text"
      },
      "source": [
        "# **Image Vector similarity code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPjiqj5xNwCD",
        "colab_type": "code",
        "outputId": "2fc904ef-e42e-4bda-9e3b-dc9f625e4774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845
        }
      },
      "source": [
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "model=VGG16(weights='imagenet', include_top=False)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, None, None, 3)     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5R3hed5Ok4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images={}\n",
        "def read_images(path):\n",
        "    for img in os.listdir(path):\n",
        "      try:\n",
        "        images[img]=image.load_img(path+img, target_size=(224, 224))\n",
        "        print(img)\n",
        "      except OSError as e:\n",
        "        continue\n",
        "    return images\n",
        "  \n",
        "def create_image_vectors(images):\n",
        "  img_vectors={}\n",
        "  for img in images.keys():\n",
        "    img_data=image.img_to_array(images[img])\n",
        "    img_data=np.expand_dims(img_data, axis=0)\n",
        "    img_data=preprocess_input(img_data)\n",
        "    vgg16_feature=model.predict(img_data)\n",
        "    vgg16_feature_np=np.array(vgg16_feature)\n",
        "    img_vectors[img]=vgg16_feature_np.flatten()\n",
        "    print(img)\n",
        "    \n",
        "  return img_vectors\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWa9bT2IOC4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images=read_images('/content/drive/My Drive/Memeify dataset/images/')\n",
        "image_vectors=create_image_vectors(images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkCb-8zgRwHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(image_vectors.keys())\n",
        "import pickle\n",
        "pickle.dump(image_vectors, open('/content/drive/My Drive/Memeify dataset/default_image_vectors.pickle', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmVJbyQkTcEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.spatial.distance import cosine\n",
        "# print(image_vectors[0].shape, image_vectors[1].shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G8x5dCLVcvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_img_vector(img_path):\n",
        "  curr_image=image.load_img(img_path, target_size=(224, 224))\n",
        "  img_data=image.img_to_array(curr_image)\n",
        "  img_data=np.expand_dims(img_data, axis=0)\n",
        "  img_data=preprocess_input(img_data)\n",
        "  vgg16_feature=model.predict(img_data)\n",
        "  vgg16_feature_np=np.array(vgg16_feature)\n",
        "  return vgg16_feature_np.flatten()\n",
        "  \n",
        "def get_category_from_img_vector(img_vector, image_vectors):\n",
        "  minimum=2\n",
        "  cat=''\n",
        "  for image_vector in image_vectors.keys():\n",
        "    curr=cosine(img_vector, image_vectors[image_vector])\n",
        "    if(curr<minimum):\n",
        "      minimum=curr\n",
        "      cat=image_vector\n",
        "      \n",
        "  return cat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBiTh9LNXx_j",
        "colab_type": "code",
        "outputId": "3bc5b119-7772-4aac-cd06-9a61fc23e9a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "vec=get_img_vector('/content/drive/My Drive/Memeify dataset/images/random_test.jpeg')\n",
        "print(get_category_from_img_vector(vec, image_vectors))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dating_site_murderer\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}